{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa569de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ShapeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    複数の手書き風図形（円、三角形、四角形）を含む画像と、\n",
    "    それに対応するラベル情報（クラス、位置、面積）を読み込む PyTorch Dataset クラス。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, label_dir, file_list):\n",
    "        \"\"\"\n",
    "        コンストラクタ\n",
    "\n",
    "        Args:\n",
    "            image_dir (str): 画像フォルダへのパス（例：\"images\"）\n",
    "            label_dir (str): ラベルファイルフォルダへのパス（例：\"labels\"）\n",
    "            file_list (List[str]): 対象の画像ファイル名（拡張子なし）のリスト（例: ['img_0001', 'img_0002']）\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.file_list = file_list\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        データセットのサイズ（サンプル数）を返す\n",
    "\n",
    "        Returns:\n",
    "            int: サンプル数（画像数）\n",
    "        \"\"\"\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        指定されたインデックスの画像とラベルを返す\n",
    "\n",
    "        Args:\n",
    "            idx (int): インデックス\n",
    "\n",
    "        Returns:\n",
    "            img (Tensor): 正規化されたグレースケール画像 [1, H, W]\n",
    "            targets (Tensor): ラベル情報 [num_shapes, 6]\n",
    "                              各行は [class_id, cx, cy, w, h, area]\n",
    "        \"\"\"\n",
    "        # ファイル名のベース（拡張子なし）を取得\n",
    "        base_name = self.file_list[idx]\n",
    "\n",
    "        # 画像とラベルファイルのパスを構築\n",
    "        img_path = os.path.join(self.image_dir, base_name + \".png\")\n",
    "        label_path = os.path.join(self.label_dir, base_name + \".txt\")\n",
    "\n",
    "        # 画像をグレースケールで読み込み（H, W）、[0, 255] → [0.0, 1.0] に変換\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE).astype(\"float32\") / 255.0\n",
    "\n",
    "        # [H, W] → [1, H, W] に次元追加（PyTorchの入力に合わせる）\n",
    "        img = torch.from_numpy(img).unsqueeze(0)\n",
    "\n",
    "        # ラベル（複数図形）を読み込んで Tensor に変換\n",
    "        targets = []\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                class_id = int(parts[0])  # クラス（0=円, 1=三角形, 2=四角形）\n",
    "                cx, cy, w, h, area = map(float, parts[1:])  # 正規化された中心座標・サイズ・面積\n",
    "                targets.append([class_id, cx, cy, w, h, area])\n",
    "\n",
    "        # [num_shapes, 6] の Tensor に変換\n",
    "        targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "        return img, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9fe83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([item[0] for item in batch], dim=0)  # 画像は同じサイズなのでstack可能\n",
    "    targets = [item[1] for item in batch]  # targetsはリストのまま\n",
    "    return imgs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319df987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoader 準備完了：train=4800枚 / val=1200枚\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "学習・検証用の DataLoader を準備するスクリプト\n",
    "分割済みの dataset/train/ と dataset/val/ を対象に Dataset を構築\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ディレクトリパス\n",
    "base_dir = \"dataset\"\n",
    "train_image_dir = os.path.join(base_dir, \"train\", \"images\")\n",
    "train_label_dir = os.path.join(base_dir, \"train\", \"labels\")\n",
    "val_image_dir = os.path.join(base_dir, \"val\", \"images\")\n",
    "val_label_dir = os.path.join(base_dir, \"val\", \"labels\")\n",
    "\n",
    "# ファイル名一覧（拡張子なし）を取得\n",
    "train_files = [os.path.splitext(f)[0] for f in os.listdir(train_image_dir)]\n",
    "val_files   = [os.path.splitext(f)[0] for f in os.listdir(val_image_dir)]\n",
    "\n",
    "# ソートしておくと順序が安定\n",
    "train_files.sort()\n",
    "val_files.sort()\n",
    "\n",
    "# Dataset を作成\n",
    "train_dataset = ShapeDataset(train_image_dir, train_label_dir, train_files)\n",
    "val_dataset   = ShapeDataset(val_image_dir, val_label_dir, val_files)\n",
    "\n",
    "# DataLoader を作成\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"✅ DataLoader 準備完了：train={len(train_dataset)}枚 / val={len(val_dataset)}枚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dce0ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class YOLOShapeDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    任意個数の図形（円・三角・四角）を検出するYOLO風の軽量ネットワーク。\n",
    "    1つの出力セルあたり B個の予測ボックスを出力。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=16, B=2, num_classes=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            S (int): 出力グリッドの分割数（S x Sセル）\n",
    "            B (int): 1セルあたりの予測ボックス数\n",
    "            num_classes (int): クラス数（今回は 3：円・三角・四角）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.num_classes = num_classes\n",
    "        self.output_dim = B * (1 + 4 + num_classes + 1)  # objectness + bbox + class probs + area\n",
    "\n",
    "        # 軽量CNNバックボーン\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),  # 入力はグレースケール画像 (1, H, W)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 256x256\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 128x128\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 64x64\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 32x32\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 16x16 → S=16 に一致\n",
    "        )\n",
    "\n",
    "        # 出力層：S×SセルごとにB個の予測（objectness + bbox + class + area）\n",
    "        self.pred_head = nn.Conv2d(256, self.output_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        入力画像から予測を出力\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): [B, 1, H, W] のグレースケール画像\n",
    "\n",
    "        Returns:\n",
    "            Tensor: [B, S, S, B, 7 + num_classes] の予測\n",
    "        \"\"\"\n",
    "        feat = self.features(x)  # [B, 256, S, S]\n",
    "        out = self.pred_head(feat)  # [B, output_dim, S, S]\n",
    "\n",
    "        B, C, S, S = out.shape\n",
    "        out = out.permute(0, 2, 3, 1).contiguous()  # [B, S, S, output_dim]\n",
    "\n",
    "        out = out.view(B, S, S, self.B, -1)  # [B, S, S, B, 6 + num_classes]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "403bbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_yolo_loss(preds, targets, S=16, B=2, num_classes=3, lambda_coord=5.0, lambda_noobj=0.5, lambda_area=1.0):\n",
    "    \"\"\"\n",
    "    YOLO形式のマルチタスク損失関数（本番用）\n",
    "    - バウンディングボックス回帰（cx, cy, w, h）\n",
    "    - クラス分類（クロスエントロピー）\n",
    "    - 面積回帰（回帰損失）\n",
    "    - objectness（二値分類）\n",
    "\n",
    "    Args:\n",
    "        preds: Tensor [B, S, S, B, 1 + 4 + num_classes + 1]\n",
    "               モデルの出力\n",
    "        targets: list[list]（各バッチごとのラベルリスト）\n",
    "               各要素は [class_id, cx, cy, w, h, area]\n",
    "        S: 出力グリッド数（S × S）\n",
    "        B: 各グリッドセルあたりの予測ボックス数\n",
    "        num_classes: 図形クラス数（円・三角形・四角形 → 3）\n",
    "        lambda_coord: bbox回帰の重み（デフォルト: 5.0）\n",
    "        lambda_noobj: objectnessが0のセルの損失重み（デフォルト: 0.5）\n",
    "        lambda_area: 面積回帰損失の重み（デフォルト: 1.0）\n",
    "\n",
    "    Returns:\n",
    "        総合損失（スカラー）\n",
    "    \"\"\"\n",
    "    \n",
    "    device = preds.device\n",
    "    batch_size = preds.shape[0]\n",
    "    loss = 0.0  # 総損失初期化\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        # 1バッチ分の空ラベルテンソルを準備（予測と同形式）\n",
    "        target_tensor = torch.zeros((S, S, B, 1 + 4 + num_classes + 1), device=device)\n",
    "\n",
    "        for t in targets[b]:\n",
    "            # クラス・位置・サイズ・面積を取得\n",
    "            class_id, cx, cy, w, h, area = t\n",
    "            class_id = int(class_id)\n",
    "\n",
    "            # 対応するグリッドセル座標を取得（整数）\n",
    "            grid_x = int(cx * S)\n",
    "            grid_y = int(cy * S)\n",
    "\n",
    "            # セル内での相対座標（0〜1）\n",
    "            gx = cx * S - grid_x\n",
    "            gy = cy * S - grid_y\n",
    "\n",
    "            if grid_x >= S or grid_y >= S:\n",
    "                continue  # 画像外には登録しない（保険）\n",
    "\n",
    "            # B個の枠のうち、最初の空きスロットに登録\n",
    "            for box in range(B):\n",
    "                if target_tensor[grid_y, grid_x, box, 0] == 0:\n",
    "                    # objectness = 1\n",
    "                    target_tensor[grid_y, grid_x, box, 0] = 1.0\n",
    "\n",
    "                    # bbox座標（セル内）\n",
    "                    target_tensor[grid_y, grid_x, box, 1:5] = torch.tensor([gx, gy, w, h], device=device)\n",
    "\n",
    "                    # クラス one-hot（例: [0,1,0]）\n",
    "                    target_tensor[grid_y, grid_x, box, 5 + class_id] = 1.0\n",
    "\n",
    "                    # 面積\n",
    "                    target_tensor[grid_y, grid_x, box, -1] = area\n",
    "                    break\n",
    "\n",
    "        # 対象バッチの予測とラベルを取得\n",
    "        pred = preds[b]     # [S, S, B, D]\n",
    "        target = target_tensor\n",
    "\n",
    "        # objectness マスク（物体あり / なし）\n",
    "        obj_mask = target[..., 0] == 1\n",
    "        noobj_mask = target[..., 0] == 0\n",
    "\n",
    "        # ---------- 損失計算 -----------\n",
    "\n",
    "        # 1. objectness（二値分類）: BCE Loss\n",
    "        bce_obj = F.binary_cross_entropy_with_logits(\n",
    "            pred[..., 0], target[..., 0], reduction='none'\n",
    "        )\n",
    "        loss += (bce_obj * obj_mask).sum() + lambda_noobj * (bce_obj * noobj_mask).sum()\n",
    "\n",
    "        # 2. bbox（回帰）: MSE Loss（物体がある場所だけ）\n",
    "        if obj_mask.any():\n",
    "            loss += lambda_coord * F.mse_loss(\n",
    "                pred[..., 1:5][obj_mask],\n",
    "                target[..., 1:5][obj_mask],\n",
    "                reduction='sum'\n",
    "            )\n",
    "\n",
    "            # 3. クラス分類: Cross Entropy（物体がある場所だけ）\n",
    "            pred_cls = pred[..., 5:5+num_classes][obj_mask]               # [N, C]\n",
    "            target_cls = target[..., 5:5+num_classes][obj_mask]           # [N, C]\n",
    "            target_cls_ids = target_cls.argmax(dim=-1)                    # [N]\n",
    "            loss += F.cross_entropy(pred_cls, target_cls_ids, reduction='sum')\n",
    "\n",
    "            # 4. 面積回帰: MSE Loss（物体がある場所だけ）\n",
    "            pred_area = pred[..., -1][obj_mask]\n",
    "            target_area = target[..., -1][obj_mask]\n",
    "            loss += lambda_area * F.mse_loss(pred_area, target_area, reduction='sum')\n",
    "\n",
    "    return loss / batch_size  # バッチ平均で正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69a80e59",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9420/2901685503.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_yolo_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1553\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1560\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9420/648946107.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m]\u001b[0m \u001b[0mの予測\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \"\"\"\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mfeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [B, 256, S, S]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_head\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [B, output_dim, S, S]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1553\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1560\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1553\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1560\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mused\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \"\"\"\n\u001b[1;32m--> 176\u001b[1;33m         return F.batch_norm(\n\u001b[0m\u001b[0;32m    177\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2510\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2512\u001b[1;33m     return torch.batch_norm(\n\u001b[0m\u001b[0;32m   2513\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2514\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "'''\n",
    "# モデル、損失関数、データセットをインポート\n",
    "from model import YOLOShapeDetector              # YOLO風の図形検出モデル\n",
    "from loss import compute_yolo_loss              # カスタム損失関数（分類・回帰・面積・objectness）\n",
    "from dataset import ShapeDataset                # 自作の図形データセットクラス\n",
    "'''\n",
    "\n",
    "# --- ハイパーパラメータ設定 ---\n",
    "S = 16                       # 出力グリッド数（S x S に分割）\n",
    "B = 2                        # 1セルあたりの予測ボックス数\n",
    "num_classes = 3              # クラス数（円・三角・四角）\n",
    "input_size = 512             # 入力画像のサイズ（縦横）\n",
    "batch_size = 8              # バッチサイズ\n",
    "num_epochs = 10              # 総エポック数\n",
    "learning_rate = 1e-3         # 学習率\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 使用デバイス\n",
    "\n",
    "# --- データセット＆ローダー作成 ---\n",
    "\n",
    "# --- モデルと最適化の定義 ---\n",
    "model = YOLOShapeDetector(S=S, B=B, num_classes=num_classes).to(device)  # モデル構築＆GPUへ転送\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)             # 最適化アルゴリズムにAdamを使用\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5) \n",
    "\n",
    "# --- ロス記録用リスト ---\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# --- 学習ループ ---\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for imgs, targets in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        preds = model(imgs)\n",
    "        loss = compute_yolo_loss(preds, targets, S=S, B=B, num_classes=num_classes)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # --- 検証ループ ---\n",
    "    model.eval()  # 推論モードに切り替え（DropoutやBatchNormを無効に）\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            preds = model(imgs)\n",
    "            loss = compute_yolo_loss(preds, targets, S=S, B=B, num_classes=num_classes)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # --- エポックごとの結果表示 ---\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# --- モデル保存 ---\n",
    "torch.save(model.state_dict(), \"shape_detector.pth\")\n",
    "print(\"✅ モデルを保存しました: shape_detector.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a80642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ログ保存（JSON）---\n",
    "with open(\"loss_log.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses\n",
    "    }, f)\n",
    "print(\"📉 ロスログを loss_log.json に保存しました\")\n",
    "\n",
    "# --- ロスの可視化（任意）---\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "print(\"📊 ロスグラフを loss_curve.png に保存しました\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
