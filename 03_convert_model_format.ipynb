{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e95a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class YOLOShapeDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    任意個数の図形（円・三角・四角）を検出するYOLO風の軽量ネットワーク。\n",
    "    1つの出力セルあたり B個の予測ボックスを出力。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=16, B=2, num_classes=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            S (int): 出力グリッドの分割数（S x Sセル）\n",
    "            B (int): 1セルあたりの予測ボックス数\n",
    "            num_classes (int): クラス数（今回は 3：円・三角・四角）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.num_classes = num_classes\n",
    "        self.output_dim = B * (1 + 4 + num_classes + 1)  # objectness + bbox + class probs + area\n",
    "\n",
    "        # 軽量CNNバックボーン\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),  # 入力はグレースケール画像 (1, H, W)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 256x256\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 128x128\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 64x64\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 32x32\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 16x16 → S=16 に一致\n",
    "        )\n",
    "\n",
    "        # 出力層：S×SセルごとにB個の予測（objectness + bbox + class + area）\n",
    "        self.pred_head = nn.Conv2d(256, self.output_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        入力画像から予測を出力\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): [B, 1, H, W] のグレースケール画像\n",
    "\n",
    "        Returns:\n",
    "            Tensor: [B, S, S, B, 7 + num_classes] の予測\n",
    "        \"\"\"\n",
    "        feat = self.features(x)  # [B, 256, S, S]\n",
    "        out = self.pred_head(feat)  # [B, output_dim, S, S]\n",
    "\n",
    "        B, C, S, S = out.shape\n",
    "        out = out.permute(0, 2, 3, 1).contiguous()  # [B, S, S, output_dim]\n",
    "\n",
    "        out = out.view(B, S, S, self.B, -1)  # [B, S, S, B, 6 + num_classes]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eccbe06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ONNX形式で保存完了: shape_detector.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp/ipykernel_5864/1700941501.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"epoch10.pth\", map_location=\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# モデル構築と重み読み込み\n",
    "model = YOLOShapeDetector(S=16, B=2, num_classes=3)\n",
    "model.load_state_dict(torch.load(\"epoch10.pth\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "# ダミー入力（例: 1枚のグレースケール画像 [1, 1, 512, 512]）\n",
    "dummy_input = torch.randn(1, 1, 512, 512)\n",
    "\n",
    "# ONNXとして保存\n",
    "torch.onnx.export(model, dummy_input, \"shape_detector.onnx\",\n",
    "                  input_names=[\"input\"], output_names=[\"output\"],\n",
    "                  opset_version=11)\n",
    "print(\"✅ ONNX形式で保存完了: shape_detector.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "738ef2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力名: input\n",
      "出力名: output\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# ONNXファイルを読み込む\n",
    "model = onnx.load(\"shape_detector.onnx\")\n",
    "\n",
    "# 入力名の確認\n",
    "for input_tensor in model.graph.input:\n",
    "    print(\"入力名:\", input_tensor.name)\n",
    "\n",
    "# 出力名の確認\n",
    "for output_tensor in model.graph.output:\n",
    "    print(\"出力名:\", output_tensor.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6b5787e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 入力名: ['input']\n",
      "📌 出力名: ['output']\n"
     ]
    }
   ],
   "source": [
    "model = onnx.load(\"shape_detector.onnx\")\n",
    "print(\"📌 入力名:\", [i.name for i in model.graph.input])\n",
    "print(\"📌 出力名:\", [o.name for o in model.graph.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1c01ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ モデルの読み込みに成功しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp/ipykernel_3960/2863949496.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"epoch10.pth\", map_location=\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# モデルのインスタンスを用意（構造が保存時と同じである必要あり）\n",
    "model = YOLOShapeDetector(S=16, B=2, num_classes=3)\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(\"epoch10.pth\", map_location=\"cpu\"))\n",
    "    print(\"✅ モデルの読み込みに成功しました\")\n",
    "except Exception as e:\n",
    "    print(\"❌ 読み込み失敗:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe0e50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight: torch.Size([16, 1, 3, 3])\n",
      "features.0.bias: torch.Size([16])\n",
      "features.1.weight: torch.Size([16])\n",
      "features.1.bias: torch.Size([16])\n",
      "features.1.running_mean: torch.Size([16])\n",
      "features.1.running_var: torch.Size([16])\n",
      "features.1.num_batches_tracked: torch.Size([])\n",
      "features.4.weight: torch.Size([32, 16, 3, 3])\n",
      "features.4.bias: torch.Size([32])\n",
      "features.5.weight: torch.Size([32])\n",
      "features.5.bias: torch.Size([32])\n",
      "features.5.running_mean: torch.Size([32])\n",
      "features.5.running_var: torch.Size([32])\n",
      "features.5.num_batches_tracked: torch.Size([])\n",
      "features.8.weight: torch.Size([64, 32, 3, 3])\n",
      "features.8.bias: torch.Size([64])\n",
      "features.9.weight: torch.Size([64])\n",
      "features.9.bias: torch.Size([64])\n",
      "features.9.running_mean: torch.Size([64])\n",
      "features.9.running_var: torch.Size([64])\n",
      "features.9.num_batches_tracked: torch.Size([])\n",
      "features.12.weight: torch.Size([128, 64, 3, 3])\n",
      "features.12.bias: torch.Size([128])\n",
      "features.13.weight: torch.Size([128])\n",
      "features.13.bias: torch.Size([128])\n",
      "features.13.running_mean: torch.Size([128])\n",
      "features.13.running_var: torch.Size([128])\n",
      "features.13.num_batches_tracked: torch.Size([])\n",
      "features.16.weight: torch.Size([256, 128, 3, 3])\n",
      "features.16.bias: torch.Size([256])\n",
      "features.17.weight: torch.Size([256])\n",
      "features.17.bias: torch.Size([256])\n",
      "features.17.running_mean: torch.Size([256])\n",
      "features.17.running_var: torch.Size([256])\n",
      "features.17.num_batches_tracked: torch.Size([])\n",
      "pred_head.weight: torch.Size([18, 256, 1, 1])\n",
      "pred_head.bias: torch.Size([18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp/ipykernel_3960/1147199462.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"epoch10.pth\", map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(\"epoch10.pth\", map_location=\"cpu\")\n",
    "for k, v in state_dict.items():\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba5c8b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 推論成功。出力サイズ: torch.Size([1, 16, 16, 2, 9])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "dummy_input = torch.randn(1, 1, 512, 512)  # 入力サイズと同じテンソル\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "    print(\"✅ 推論成功。出力サイズ:\", output[\"output\"].shape if isinstance(output, dict) else output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088830fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[ 5.1458,  1.1833,  1.0737,  ..., -2.5919, -2.8942, -0.1407],\n",
      "           [-6.2964,  1.9910,  1.1692,  ..., -5.1240, -4.8229, -3.3993]],\n",
      "\n",
      "          [[ 8.4656,  0.7383,  0.9216,  ...,  0.7699, -3.2333, -0.1409],\n",
      "           [-0.1202,  1.2647,  0.7411,  ..., -4.2575, -3.4888, -2.7104]],\n",
      "\n",
      "          [[10.0604,  0.7015,  0.9796,  ...,  0.9111, -3.3052, -0.1413],\n",
      "           [ 0.4313,  1.1894,  0.7369,  ..., -4.6759, -3.7304, -2.7940]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[ 9.0369,  0.7060,  0.9295,  ...,  0.0407, -2.7553, -0.1235],\n",
      "           [ 0.3575,  1.1347,  0.7501,  ..., -4.4404, -3.3162, -2.5226]],\n",
      "\n",
      "          [[ 9.3854,  0.6630,  0.9838,  ...,  0.4747, -3.2442, -0.1290],\n",
      "           [ 0.1025,  1.1470,  0.6961,  ..., -4.7163, -3.5064, -2.7031]],\n",
      "\n",
      "          [[ 5.3006,  0.1975,  1.0837,  ...,  3.8709, -3.6939, -0.1458],\n",
      "           [-5.1387,  0.9585,  1.3445,  ..., -4.7916, -2.6800, -2.9720]]],\n",
      "\n",
      "\n",
      "         [[[ 9.1355,  1.1770,  0.4572,  ..., -5.0513, -3.0504, -0.1337],\n",
      "           [ 0.7753,  1.3184,  1.3852,  ..., -5.7100, -3.5655, -2.9135]],\n",
      "\n",
      "          [[11.3476,  0.6647,  0.2016,  ..., -2.6616, -3.9803, -0.1305],\n",
      "           [ 4.8132,  0.5697,  1.0311,  ..., -5.6141, -2.1687, -2.0919]],\n",
      "\n",
      "          [[11.2010,  0.6948,  0.2373,  ..., -2.5094, -3.9998, -0.1316],\n",
      "           [ 4.6572,  0.5888,  1.0905,  ..., -5.6630, -2.2118, -2.1423]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[10.8184,  0.6942,  0.2134,  ..., -2.4795, -3.9670, -0.1284],\n",
      "           [ 4.5954,  0.5445,  1.0511,  ..., -5.5401, -2.0593, -2.0531]],\n",
      "\n",
      "          [[11.2966,  0.6742,  0.2583,  ..., -3.0370, -3.7620, -0.1305],\n",
      "           [ 4.8789,  0.5946,  1.0253,  ..., -5.5783, -2.1448, -1.9033]],\n",
      "\n",
      "          [[ 9.0706,  0.3142,  0.3045,  ...,  1.8381, -4.3358, -0.1323],\n",
      "           [ 0.1265,  0.4986,  1.6499,  ..., -5.2475, -1.5040, -2.5020]]],\n",
      "\n",
      "\n",
      "         [[[ 7.8582,  1.1445,  0.3987,  ..., -4.4744, -3.3304, -0.1219],\n",
      "           [ 0.3772,  1.3254,  1.4692,  ..., -5.4853, -3.3248, -2.8019]],\n",
      "\n",
      "          [[10.7595,  0.6591,  0.1923,  ..., -2.3895, -4.1408, -0.1273],\n",
      "           [ 4.5429,  0.5772,  1.0125,  ..., -5.7171, -2.0596, -2.0790]],\n",
      "\n",
      "          [[11.7168,  0.6444,  0.2576,  ..., -3.1118, -3.8878, -0.1322],\n",
      "           [ 5.0625,  0.6055,  1.0093,  ..., -5.7454, -2.0364, -1.9825]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[11.3473,  0.6924,  0.2689,  ..., -2.7552, -3.9059, -0.1336],\n",
      "           [ 4.8059,  0.5526,  1.0021,  ..., -5.4724, -2.0349, -2.0758]],\n",
      "\n",
      "          [[10.9783,  0.6352,  0.2308,  ..., -3.1256, -3.7409, -0.1309],\n",
      "           [ 4.7493,  0.5391,  0.9930,  ..., -5.5969, -1.8884, -1.9134]],\n",
      "\n",
      "          [[ 8.5255,  0.3056,  0.3150,  ...,  1.9523, -4.1884, -0.1369],\n",
      "           [-0.0311,  0.4951,  1.6664,  ..., -5.1567, -1.4247, -2.4602]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 9.1676,  1.2331,  0.4575,  ..., -5.6925, -3.0424, -0.1313],\n",
      "           [ 0.8486,  1.3580,  1.4445,  ..., -5.6698, -3.5621, -3.0211]],\n",
      "\n",
      "          [[11.1341,  0.6502,  0.2071,  ..., -2.8420, -3.7879, -0.1287],\n",
      "           [ 4.8409,  0.5479,  1.0372,  ..., -5.7175, -1.9084, -1.9603]],\n",
      "\n",
      "          [[11.1179,  0.6798,  0.2365,  ..., -2.7146, -3.6895, -0.1287],\n",
      "           [ 4.7973,  0.6209,  1.0612,  ..., -5.5293, -1.9696, -2.0001]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[10.8701,  0.6622,  0.2547,  ..., -3.0514, -3.8179, -0.1280],\n",
      "           [ 4.7193,  0.5868,  1.0239,  ..., -5.6386, -2.0002, -1.9171]],\n",
      "\n",
      "          [[11.6055,  0.6605,  0.2484,  ..., -2.7166, -3.8308, -0.1311],\n",
      "           [ 5.0447,  0.5635,  1.0143,  ..., -5.6646, -1.9767, -1.9364]],\n",
      "\n",
      "          [[ 9.0724,  0.2775,  0.3680,  ...,  2.3499, -4.3969, -0.1435],\n",
      "           [-0.0459,  0.5739,  1.7051,  ..., -5.1904, -1.4954, -2.5326]]],\n",
      "\n",
      "\n",
      "         [[[ 9.2674,  1.2043,  0.4912,  ..., -5.4341, -3.1947, -0.1398],\n",
      "           [ 0.8224,  1.4060,  1.4037,  ..., -5.8599, -3.6572, -3.0642]],\n",
      "\n",
      "          [[11.2805,  0.6821,  0.2159,  ..., -2.1266, -3.9413, -0.1306],\n",
      "           [ 4.7046,  0.5936,  0.9921,  ..., -5.4188, -2.1717, -2.0902]],\n",
      "\n",
      "          [[10.9815,  0.6568,  0.2391,  ..., -2.5316, -4.0297, -0.1235],\n",
      "           [ 4.4172,  0.5551,  1.0164,  ..., -5.5858, -2.0963, -2.1509]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[10.6496,  0.6722,  0.1588,  ..., -2.1862, -3.8940, -0.1190],\n",
      "           [ 4.3806,  0.5912,  1.0509,  ..., -5.4315, -2.0798, -2.1920]],\n",
      "\n",
      "          [[11.0835,  0.7132,  0.2521,  ..., -2.4811, -4.0891, -0.1300],\n",
      "           [ 4.5497,  0.6192,  1.0918,  ..., -5.6067, -2.1704, -2.0459]],\n",
      "\n",
      "          [[ 9.0349,  0.2807,  0.3405,  ...,  2.6342, -4.5193, -0.1438],\n",
      "           [-0.5708,  0.5680,  1.7141,  ..., -5.2217, -1.5570, -2.6618]]],\n",
      "\n",
      "\n",
      "         [[[ 8.5409,  1.6750, -0.2816,  ...,  2.3524, -3.3960, -0.1999],\n",
      "           [-3.7466,  0.8845,  1.9510,  ..., -3.7184, -4.1763, -3.7259]],\n",
      "\n",
      "          [[ 9.0426,  0.9182, -0.4346,  ...,  2.9788, -4.0858, -0.1887],\n",
      "           [ 1.1305,  0.5722,  1.2098,  ..., -3.8750, -2.4106, -2.7328]],\n",
      "\n",
      "          [[ 8.4236,  0.9315, -0.4924,  ...,  2.6397, -4.2180, -0.1895],\n",
      "           [ 0.3480,  0.5594,  1.1504,  ..., -4.0917, -2.5931, -2.7983]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[ 8.8937,  0.9637, -0.4459,  ...,  2.5979, -3.9346, -0.1822],\n",
      "           [ 0.7920,  0.5341,  1.1311,  ..., -3.9412, -2.5777, -2.7024]],\n",
      "\n",
      "          [[ 9.1231,  0.9031, -0.4659,  ...,  1.8724, -4.1348, -0.1864],\n",
      "           [ 1.2505,  0.5113,  1.1504,  ..., -4.4860, -2.3987, -2.6859]],\n",
      "\n",
      "          [[ 9.4985,  0.3352, -0.5157,  ...,  5.0876, -4.9478, -0.1991],\n",
      "           [-2.4726,  0.3702,  1.6657,  ..., -4.2565, -1.6271, -2.9990]]]]])\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5601d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ONNX変換も成功\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, dummy_input, \"check.onnx\",\n",
    "                  input_names=[\"input\"], output_names=[\"output\"],\n",
    "                  opset_version=11)\n",
    "print(\"✅ ONNX変換も成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959ece3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
